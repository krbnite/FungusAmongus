{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treebeard and the Fungus Amongus\n",
    "The data we explore today (whatever day it is for you) is the [mushroom data](https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/) found at the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).  \n",
    "\n",
    "The type of exploration, you ask?  \n",
    "\n",
    "\n",
    "\n",
    "![Treebeard](./assets/Treebeard.jpg)\n",
    "\n",
    "Asking sentient trees what they know about poisonous mushrooms, of course. \n",
    "\n",
    "In the process, maybe we will learn a thing or two about these \n",
    "[arboreal decision makers](http://scikit-learn.org/stable/modules/tree.html#tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-08-01 15:25:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names\n",
      "Resolving archive.ics.uci.edu... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6816 (6.7K) [text/plain]\n",
      "Saving to: ‘agaricus-lepiota.names.1’\n",
      "\n",
      "agaricus-lepiota.na 100%[===================>]   6.66K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2017-08-01 15:25:56 (6.56 MB/s) - ‘agaricus-lepiota.names.1’ saved [6816/6816]\n",
      "\n",
      "--2017-08-01 15:25:56--  https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
      "Resolving archive.ics.uci.edu... 128.195.10.249\n",
      "Connecting to archive.ics.uci.edu|128.195.10.249|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 373704 (365K) [text/plain]\n",
      "Saving to: ‘agaricus-lepiota.data.1’\n",
      "\n",
      "agaricus-lepiota.da 100%[===================>] 364.95K   464KB/s    in 0.8s    \n",
      "\n",
      "2017-08-01 15:25:58 (464 KB/s) - ‘agaricus-lepiota.data.1’ saved [373704/373704]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Mushroom Data\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.names\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\n",
    "\n",
    "# Extract lines w/ feature names\n",
    "#  -- you might use python to do this; I just use whatever quickly comes to mind\n",
    "!cat agaricus-lepiota.names | grep \"^[[:space:]]\\{4,5\\}[0-9]\\{1,2\\}.*:\" > feature_names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_selection as filt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data with [pd.read_csv](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html),\n",
    "making sure to specify the names. (Simple inspection of the file shows that the target variable is in the first column, followed by the features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract features names\n",
    "with open('feature_names.txt') as f:\n",
    "    features = [line.split()[1][0:-1] for line in f]\n",
    "    \n",
    "# Get Data\n",
    "data = pd.read_csv('agaricus-lepiota.data', names=['deadly']+features)\n",
    "y = pd.DataFrame([1 if target=='p' else 0 for target in data['deadly']], columns=['deadly'])\n",
    "x = data.drop('deadly', axis=1)\n",
    "\n",
    "# Train, Validate, Test\n",
    "x_trn, x_vt, y_trn, y_vt = train_test_split(x, y, train_size=0.70)\n",
    "x_val, x_tst, y_val, y_tst = train_test_split(x_vt, y_vt, test_size=0.50)\n",
    "\n",
    "# Some/Most techniques require the categorical vars to be one-hot encoded\n",
    "x_trn_code = pd.get_dummies(x_trn)\n",
    "x_val_code = pd.get_dummies(x_val)\n",
    "x_tst_code = pd.get_dummies(x_tst)\n",
    "\n",
    "# Make sure the various dummy vars are represented in each subset\n",
    "all_ftrs = pd.get_dummies(x).columns\n",
    "for ftr in all_ftrs.difference(x_trn_code.columns): x_trn_code[ftr]=0\n",
    "for ftr in all_ftrs.difference(x_val_code.columns): x_val_code[ftr]=0\n",
    "for ftr in all_ftrs.difference(x_tst_code.columns): x_tst_code[ftr]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are these classes balanced or imbalanced?\n",
    "Turns out they're pretty dang balanced, which means we don't have to think too much.\n",
    "But beware: rarely are classes balanced in the \"real\" world.  (No, mushrooms are not real!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deadly    0.479775\n",
       "dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trn.sum() / y_trn.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Tree\n",
    "Interestingly, from a theoretical perspective, decision trees can handle categorical\n",
    "variables just fine without one-hot encoding them.  However, [from an implementation perspective,\n",
    "this is not necessarily true](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/). Turns out, one-hot encoding can degrade the performance (beware the curse of dimensionality!).  But [scikit-learn don't care](https://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest)!  In scikit-learn, you **have** to one-hot encode the categorical features.  \n",
    "\n",
    "Note that this is a bummer since one of the desirable traits of decision trees is the fact that they require almost no preprocessing (no need for logs or Box-Cox, normalization, etc).  So, one thing to add to this project is to also do a decision tree (and random forest) in R to see how the results differ.\n",
    "\n",
    "In sklearn, we have various options for a [decision tree](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier):\n",
    "* criterion: \n",
    "    - Default: 'gini' (Gini impurity)\n",
    "    - 'entropy' (information gain)\n",
    "* splitter: \n",
    "    - Default: 'best'\n",
    "    - 'random'\n",
    "* max_features\n",
    "    - Default: None (max_features = n_features)\n",
    "    - int (consider int features at each split), e.g., 3\n",
    "    - float (percentage of total features considered at each split), e.g., 0.5\n",
    "    - 'auto' (max_features = sqrt(n_features))\n",
    "    - 'sqrt' (same as 'auto')\n",
    "    - 'log2' (can you guess?)\n",
    "* max_depth\n",
    "    - Default: None (go deep!)\n",
    "    - int: do not go further than int splits\n",
    "    - if None, splits continue until all leaves are pure or min_samples_split is reached\n",
    "* min_samples_split\n",
    "    - Default: 2 (i.e., split until leaf is pure or some other constraint is met)\n",
    "    - int (split if n_data_points_in_node >= int, barring no other constraint prevents such)\n",
    "    - float (split if n_data_points_in_node >= ceil(float\\*n_data_points), barring no other constraint)\n",
    "* min_samples_leaf\n",
    "    - \n",
    "* min_weight_fraction_leaf\n",
    "* max_leaf_nodes\n",
    "* class_weight\n",
    "* random_state\n",
    "* min_impurity_split\n",
    "\n",
    "Whoa, all these options... It's like ice cream or beer: too many choices, damn it!\n",
    "\n",
    "No worries... Let's just play around with them.  Maybe we will learn something!  \n",
    "(Disclaimer: Or maybe not.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini: 0.47973713034\n",
      "Entropy: 0.617096018735\n"
     ]
    }
   ],
   "source": [
    "gin_tree = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "gin_tree.fit(x_trn_code, y_trn)\n",
    "ent_tree.fit(x_trn_code, y_trn)\n",
    "print('Gini:',    metrics.f1_score(y_val, gin_tree.predict(x_val_code)))\n",
    "print('Entropy:', metrics.f1_score(y_val, ent_tree.predict(x_val_code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a \"bootstrapping\" of sorts by changing the random_state and finding the median. (Obviously this is ok for this small data set...but becomes less ok as N -> BIG!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probably a more efficient way of doing this than forLoop, but uhhh... Look over there! -->\n",
    "gf1=list(); ef1=list()\n",
    "for i in range(1000):\n",
    "    gin_tree = DecisionTreeClassifier(random_state=i, criterion='gini')\n",
    "    ent_tree = DecisionTreeClassifier(random_state=i, criterion='entropy')\n",
    "    gin_tree.fit(x_trn_code, y_trn)\n",
    "    ent_tree.fit(x_trn_code, y_trn)\n",
    "    gf1.append(metrics.f1_score(y_val, gin_tree.predict(x_val_code)))\n",
    "    ef1.append(metrics.f1_score(y_val, ent_tree.predict(x_val_code)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e93f060da1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mef1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(gf1).hist()\n",
    "pd.DataFrame(ef1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x11a4f72b0>]], dtype=object)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEICAYAAAC3Y/QeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0kNMvwlqUGGvyQ1yPCXpAYZ/pLUoP8PX7u1xGzy+PYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a5869e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(ef1).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will Feature Selection Help?\n",
    "Oftentimes feature selection is used with at least two goals in mind:\n",
    "- discard features that have almost no \"information value\" with regard to the target\n",
    "- remove collinearity among the features\n",
    "\n",
    "Both are meant to address various issues, such as the curse of dimensionality and \n",
    "overfitting.  Removing collinearity in some models is essential, while in other\n",
    "it is just helpful.  Removing seemingly useless variables helps hedge against \n",
    "fitting to noise.  \n",
    "\n",
    "Anyway, the point is feature selection (here synonymous with dimensionality reduction)\n",
    "is often an important step for improving model performance in various ways. How it might\n",
    "help with decision trees is something we will address.\n",
    "\n",
    "More generally, one might what wonder which issue to address first.  My answer is \n",
    "to first reduce your feature space by discarding variables that seem to have little\n",
    "to say about the target.  For example, say you have N features and you \n",
    "want to (a) order your features by\n",
    "how well they individually correlate with the target, and (b) identify groups of\n",
    "highly correlated features to select single representatives from.  \n",
    "\n",
    "A: If you choose step (a) then (b), you will (1) compute N correlations with target, keeping only \n",
    "M <= N; and (2) compute (M-1)+(M-2)+...+1 correlations.  \n",
    "\n",
    "B: If you choose step (b) then (a), you will (1) compute (N-1)+(N-2)+...+1 correlations,\n",
    "and choose to keep P <= N features; and (2) compute P correlations with target.\n",
    "\n",
    "In scenario A, you will always compute at least N correlations and, at most,\n",
    "N+(N-1)+...+1 correlations (discard no features). \n",
    "\n",
    "In scenario B, you will always compute at least (N-1)+(N-2)+...+1 correlations. \n",
    "\n",
    "\n",
    "## Pairwise Correlation Filters\n",
    "This is sometimes referred to as univariate feature selection. I like to think\n",
    "about it as filtering a list of features, allowing only those with certain desirable\n",
    "qualities to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kurban/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [82] are constant.\n",
      "  UserWarning)\n",
      "/Users/kurban/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "num_ftrs = 8\n",
    "chi2_filt     = filt.SelectKBest(filt.chi2, k=num_ftrs).fit(x_trn_code, y_trn.squeeze())\n",
    "chi2_ftrs = x_trn_code.columns[chi2_filt.get_support()]\n",
    "\n",
    "fval_filt     = filt.SelectKBest(filt.f_classif, k=num_ftrs).fit(x_trn_code, y_trn.squeeze())\n",
    "fval_ftrs = x_trn_code.columns[fval_filt.get_support()]\n",
    "\n",
    "info_filt     = filt.SelectKBest(filt.mutual_info_classif, k=num_ftrs).fit(x_trn_code, y_trn.squeeze())\n",
    "info_ftrs = x_trn_code.columns[info_filt.get_support()]\n",
    "\n",
    "best_ftrs = fval_ftrs.intersection(chi2_ftrs).intersection(info_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The important features to consider when deciding whether or not to eat that mushroom:\n",
      "\t odor_f\n",
      "\t odor_n\n",
      "\t gill-size_n\n",
      "\t gill-color_b\n",
      "\t stalk-surface-above-ring_k\n",
      "\t stalk-surface-below-ring_k\n"
     ]
    }
   ],
   "source": [
    "print('The important features to consider when deciding whether or not to eat that mushroom:')\n",
    "_ = [print(\"\\t\",ftr) for ftr in best_ftrs.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini: 0.979389942292\n",
      "Entropy: 0.979389942292\n"
     ]
    }
   ],
   "source": [
    "gin_tree = DecisionTreeClassifier(random_state=0, criterion='gini')\n",
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "gin_tree.fit(x_trn_code[best_ftrs], y_trn)\n",
    "ent_tree.fit(x_trn_code[best_ftrs], y_trn)\n",
    "print('Gini:',    metrics.f1_score(y_val, gin_tree.predict(x_val_code[best_ftrs])))\n",
    "print('Entropy:', metrics.f1_score(y_val, ent_tree.predict(x_val_code[best_ftrs])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Feature selection helped a lot.  So much so that it doesn't even matter if Treebeard makes decisions \n",
    "using the Gini or Entropy criterion!\n",
    "\n",
    "I found that choosing the 12 best features for each filter had an intersection of 8, and identical\n",
    "F1 scores for the Gini and Entropy criteria (just over 0.979).  \n",
    "\n",
    "Interestingly, I found that choosing the 8 best features had an intersection of 6 features, and the \n",
    "same F1 scores.  \n",
    "\n",
    "However, if one chooses to look for the 7 best features, the 3 filters have an intersection of only 5 features and \n",
    "the F1 scores drop to just above 0.92 (though still identical, believe it or not).\n",
    "\n",
    "### If we couldn't take the intersection of multiple feature filters, which would have done best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy (No Filter): 0.617096018735\n"
     ]
    }
   ],
   "source": [
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(x_trn_code, y_trn)\n",
    "print('Entropy (No Filter):', metrics.f1_score(y_val, ent_tree.predict(x_val_code)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi2 Entropy: 0.979389942292\n",
      "\n",
      "Features not in the Best Set\n",
      "\t ring-type_l\n",
      "\t spore-print-color_h\n"
     ]
    }
   ],
   "source": [
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(x_trn_code[chi2_ftrs], y_trn)\n",
    "print('Chi2 Entropy:', metrics.f1_score(y_val, ent_tree.predict(x_val_code[chi2_ftrs])))\n",
    "print('\\nFeatures not in the Best Set')\n",
    "_ = [print(\"\\t\",ftr) for ftr in chi2_ftrs.difference(best_ftrs).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Value Entropy: 0.979389942292\n",
      "\n",
      "Features not in the Best Set\n",
      "\t gill-size_b\n",
      "\t ring-type_p\n"
     ]
    }
   ],
   "source": [
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(x_trn_code[fval_ftrs], y_trn)\n",
    "print('F-Value Entropy:', metrics.f1_score(y_val, ent_tree.predict(x_val_code[fval_ftrs])))\n",
    "print('\\nFeatures not in the Best Set')\n",
    "_ = [print(\"\\t\",ftr) for ftr in fval_ftrs.difference(best_ftrs).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Entropy: 0.979149291076\n",
      "\n",
      "Features not in the Best Set\n",
      "\t gill-size_b\n",
      "\t ring-type_p\n"
     ]
    }
   ],
   "source": [
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(x_trn_code[info_ftrs], y_trn)\n",
    "print('Info Entropy:', metrics.f1_score(y_val, ent_tree.predict(x_val_code[info_ftrs])))\n",
    "print('\\nFeatures not in the Best Set')\n",
    "_ = [print(\"\\t\",ftr) for ftr in info_ftrs.difference(best_ftrs).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity: Seek and Destroy?\n",
    "\n",
    "1. Create feature corr matrix\n",
    "2. Mask: Surface only those (row,col) pairs w/ \n",
    "    - corr > threshold\n",
    "    - corrs != 1.0000 (the (i,i) pairs along the diagonal)\n",
    "3. Convert matrix into (row_name, col_name, corr_value) representation\n",
    "    - this can be done using the [stack](https://stackoverflow.com/questions/26854091/getting-index-column-pairs-for-true-elements-of-a-boolean-dataframe-in-pandas) method, which turns the DataFrame into a multi-index Series, and then [resetting the Series index](https://stackoverflow.com/questions/20110170/turn-pandas-multi-index-into-column) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_corr = x_trn_code[best_ftrs.drop(['odor_f','gill-size_n', 'stalk-surface-above-ring_k'])].corr().abs()\n",
    "high_corrs = x_corr[(x_corr >= 0.5) & (x_corr != 1.000000)].\\\n",
    "    stack().\\\n",
    "    reset_index().\\\n",
    "    rename(columns={'level_0': 'row', 'level_1': 'col', 0: 'val'}).\\\n",
    "    drop_duplicates('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>col</th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row, col, val]\n",
       "Index: []"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "high_corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bf = best_ftrs.drop(['odor_f','gill-size_n', 'stalk-surface-above-ring_k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Entropy: 0.874418604651\n"
     ]
    }
   ],
   "source": [
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(x_trn_code[bf], y_trn)\n",
    "print('Info Entropy:', metrics.f1_score(y_val, ent_tree.predict(x_val_code[bf])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "You have to ask: why do I care about collinearity?  The reason would be because you deeply care about understanding the effect of each feature independently of all other features.  But if you care only about prediction performance, \n",
    "there is no need to care too much about collinearity.  Sure, some variables share a lot of the same info about the target, but they also may contain small bits of independent info as well.  So if you don't need to explain every detail about your model, keep 'em all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To hell with understanding: Let's Predict the Future!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Entropy: 0.979149291076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(1).fit(x_trn_code[best_ftrs], y_trn)\n",
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(pca.transform(x_trn_code[best_ftrs]), y_trn)\n",
    "print('Info Entropy:', \\\n",
    "      metrics.f1_score(y_val, ent_tree.predict(pca.transform(x_val_code[best_ftrs]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should basically be thinking, \"Oh my God! That can't be real.\"\n",
    "\n",
    "We've literally reduced the 100+ dimensions of the input feature space to a single dimension, and it performs\n",
    "just as well as our 6 best features.  That is amazing.\n",
    "\n",
    "Would PCA have worked out so well if we had done this from the beginning?  \n",
    "\n",
    "Absolutely not.  PCA is an unsupervised dimensionality reduction technique that equates feature variance \n",
    "with feature importance.  This consideration is independent of any information regarding the target.  \n",
    "PCA worked so well this time solely because we first reduced the feature space to the 6 most essential\n",
    "features.  In other words, we threw out any noise and irrelevant information that would have \n",
    "rendered PCA useless.  (Rotten icing on the moldy cake: Even if PCA did not suffer these issues, it would have been more computationally demanding to do it first. )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Entropy: 0.291245791246\n"
     ]
    }
   ],
   "source": [
    "# Show the kids how blindly applying PCA would have worked out\n",
    "pca = PCA(1).fit(x_trn_code, y_trn)\n",
    "ent_tree = DecisionTreeClassifier(random_state=0, criterion='entropy')\n",
    "ent_tree.fit(pca.transform(x_trn_code), y_trn)\n",
    "print('Info Entropy:', \\\n",
    "      metrics.f1_score(y_val, ent_tree.predict(pca.transform(x_val_code))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
